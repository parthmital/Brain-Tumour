{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0) Environment Setup","metadata":{}},{"cell_type":"code","source":"!pip -q install nibabel tqdm torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1) Imports and Global State","metadata":{}},{"cell_type":"code","source":"import os, glob, random, logging, sys, time\nimport numpy as np\nimport nibabel as nib\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nos.makedirs(\"/kaggle/working/logs\", exist_ok=True)\nlogging.basicConfig(\n    level=logging.INFO, \n    format=\"%(asctime)s | %(levelname)s | %(message)s\", \n    handlers=[\n        logging.FileHandler(\"/kaggle/working/logs/training.log\"),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\nLOG = logging.getLogger(\"BraTS_Fast_nnUNet\")\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nROOT = \"/kaggle/input/brats20-dataset-training-validation\"\nTRAIN_BASE = f\"{ROOT}/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\nVAL_BASE = f\"{ROOT}/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData\"\n\nLOG.info(f\"Initialized: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2) Data Discovery","metadata":{}},{"cell_type":"code","source":"def find_cases(base, is_val=False):\n    cases = []\n    pattern = \"BraTS20_Validation_*\" if is_val else \"BraTS20_Training_*\"\n    found_dirs = sorted(glob.glob(os.path.join(base, pattern)))\n    \n    for d in found_dirs:\n        def check_file(p):\n            found = glob.glob(os.path.join(d, p))\n            return found[0] if found else None\n            \n        m = {\n            \"flair\": check_file(\"*_flair.nii*\"), \n            \"t1\": check_file(\"*_t1.nii*\"),\n            \"t1ce\": check_file(\"*_t1ce.nii*\"), \n            \"t2\": check_file(\"*_t2.nii*\"),\n            \"seg\": check_file(\"*_seg.nii*\")\n        }\n        \n        if not is_val and all(v is not None for v in m.values()):\n            cases.append((os.path.basename(d), m))\n        elif is_val and all(m[k] is not None for k in [\"flair\", \"t1\", \"t1ce\", \"t2\"]):\n            cases.append((os.path.basename(d), m))\n            \n    return cases\n\ntrain_cases = find_cases(TRAIN_BASE, is_val=False)\nval_cases = find_cases(VAL_BASE, is_val=True)\nLOG.info(f\"Dataset Structure: {len(train_cases)} Train | {len(val_cases)} Val\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3) Training Configuration","metadata":{}},{"cell_type":"code","source":"PATCH_SIZE = (128, 128, 128)\nBATCH_SIZE = 12\nEPOCHS = 4\nLR_START = 0.01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4) Dataset with Foreground-Biased Sampling","metadata":{}},{"cell_type":"code","source":"class FastBraTSDataset(Dataset):\n    def __init__(self, cases, samples=1000):\n        self.cases = cases\n        self.samples = samples\n\n    def __len__(self): return self.samples\n\n    def __getitem__(self, _):\n        cid, mf = random.choice(self.cases)\n        imgs = []\n        for k in [\"flair\",\"t1\",\"t1ce\",\"t2\"]:\n            data = nib.load(mf[k]).get_fdata().astype(np.float32)\n            mask = data != 0\n            if mask.sum() > 10:\n                data[mask] = (data[mask] - data[mask].mean()) / (data[mask].std() + 1e-8)\n            imgs.append(data)\n        x = np.stack(imgs)\n        \n        seg = nib.load(mf[\"seg\"]).get_fdata()\n        y = np.stack([(seg > 0), ((seg == 1) | (seg == 4)), (seg == 4)]).astype(np.float32)\n\n        d, h, w = x.shape[1:]\n        pd, ph, pw = PATCH_SIZE\n        \n        if random.random() < 0.66:\n            z, i, j = np.where(y[0] > 0)\n            if len(z) > 0:\n                idx = random.randint(0, len(z)-1)\n                sd = np.clip(z[idx] - pd//2, 0, d-pd)\n                sh = np.clip(i[idx] - ph//2, 0, h-ph)\n                sw = np.clip(j[idx] - pw//2, 0, w-pw)\n            else:\n                sd, sh, sw = random.randint(0, d-pd), random.randint(0, h-ph), random.randint(0, w-pw)\n        else:\n            sd, sh, sw = random.randint(0, d-pd), random.randint(0, h-ph), random.randint(0, w-pw)\n\n        x_p, y_p = x[:, sd:sd+pd, sh:sh+ph, sw:sw+pw], y[:, sd:sd+pd, sh:sh+ph, sw:sw+pw]\n        \n        for axis in [1, 2, 3]:\n            if random.random() > 0.5:\n                x_p, y_p = np.flip(x_p, axis).copy(), np.flip(y_p, axis).copy()\n                \n        return torch.from_numpy(x_p), torch.from_numpy(y_p)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5) Model Architecture (nnU-Net with Deep Supervision)","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, i, o):\n        super().__init__()\n        self.c = nn.Sequential(\n            nn.Conv3d(i, o, 3, 1, 1, bias=False),\n            nn.InstanceNorm3d(o),\n            nn.LeakyReLU(0.01, True),\n            nn.Dropout3d(0.2)\n        )\n    def forward(self, x): return self.c(x)\n\nclass nnUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = [32, 64, 128, 256]\n        self.e1, self.e2, self.e3 = ConvBlock(4, f[0]), ConvBlock(f[0], f[1]), ConvBlock(f[1], f[2])\n        self.pool = nn.MaxPool3d(2)\n        self.bottleneck = ConvBlock(f[2], f[3])\n        self.u3, self.u2, self.u1 = nn.ConvTranspose3d(f[3], f[2], 2, 2), nn.ConvTranspose3d(f[2], f[1], 2, 2), nn.ConvTranspose3d(f[1], f[0], 2, 2)\n        self.d3, self.d2, self.d1 = ConvBlock(f[3], f[2]), ConvBlock(f[2], f[1]), ConvBlock(f[1], f[0])\n        self.ds3 = nn.Conv3d(f[2], 3, 1)\n        self.ds2 = nn.Conv3d(f[1], 3, 1)\n        self.out = nn.Conv3d(f[0], 3, 1)\n\n    def forward(self, x):\n        s1 = self.e1(x); s2 = self.e2(self.pool(s1)); s3 = self.e3(self.pool(s2))\n        b = self.bottleneck(self.pool(s3))\n        d3 = self.d3(torch.cat([self.u3(b), s3], 1))\n        d2 = self.d2(torch.cat([self.u2(d3), s2], 1))\n        d1 = self.d1(torch.cat([self.u1(d2), s1], 1))\n        return self.out(d1), self.ds2(d2), self.ds3(d3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6) Hybrid Loss Function","metadata":{}},{"cell_type":"code","source":"def hybrid_loss(outputs, target):\n    def region_loss(p, y):\n        p_sig = torch.sigmoid(p)\n        dice = 1 - (2*(p_sig*y).sum()+1e-6)/(p_sig.sum()+y.sum()+1e-6)\n        bce = F.binary_cross_entropy_with_logits(p, y)\n        return dice + bce\n    \n    l1 = region_loss(outputs[0], target)\n    l2 = region_loss(outputs[1], F.interpolate(target, outputs[1].shape[2:], mode='nearest'))\n    l3 = region_loss(outputs[2], F.interpolate(target, outputs[2].shape[2:], mode='nearest'))\n    return l1 + 0.5*l2 + 0.25*l3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7) Training Loop with Poly LR Scheduler","metadata":{}},{"cell_type":"code","source":"if len(train_cases) > 0:\n    train_split, internal_val = train_test_split(train_cases, test_size=0.1, random_state=SEED)\n    train_loader = DataLoader(FastBraTSDataset(train_split), batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(FastBraTSDataset(internal_val, samples=20), batch_size=2)\n\n    net = nn.DataParallel(nnUNet()).to(DEVICE)\n    optimizer = torch.optim.SGD(net.parameters(), lr=LR_START, momentum=0.99, nesterov=True)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ep: (1 - ep/EPOCHS)**0.9)\n    scaler = torch.amp.GradScaler(enabled=True)\n\n    for epoch in range(EPOCHS):\n        net.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        \n        for x, y in pbar:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            with torch.amp.autocast(device_type=\"cuda\"):\n                outputs = net(x)\n                loss = hybrid_loss(outputs, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        net.eval()\n        torch.cuda.empty_cache()\n        d_scores = []\n        with torch.no_grad():\n            for vx, vy in val_loader:\n                vx = vx.to(DEVICE)\n                with torch.amp.autocast(device_type=\"cuda\"):\n                    v_out, _, _ = net(vx)\n                p = (torch.sigmoid(v_out) > 0.5).float().cpu()\n                d_scores.append((2.*(p*vy).sum()/(p.sum()+vy.sum()+1e-6)).item())\n        \n        LOG.info(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val Dice: {np.mean(d_scores):.4f}\")\n        scheduler.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8) Uncertainty Inference Function","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef infer_uncertainty(x, model, mc_iter=8):\n    model.train()\n    preds = [torch.sigmoid(model(x)[0]) for _ in range(mc_iter)]\n    mean_p = torch.stack(preds).mean(0)\n    uncertainty = - (mean_p * torch.log(mean_p + 1e-8) + (1-mean_p) * torch.log(1-mean_p + 1e-8))\n    return mean_p, (uncertainty / np.log(2) * 100).clamp(0, 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9) Model Export","metadata":{}},{"cell_type":"code","source":"SAVE_FILE = \"/kaggle/working/weights/nnunet_brats2020.pth\"\nos.makedirs(os.path.dirname(SAVE_FILE), exist_ok=True)\ntry:\n    if isinstance(net, nn.DataParallel):\n        torch.save(net.module.state_dict(), SAVE_FILE)\n    else:\n        torch.save(net.state_dict(), SAVE_FILE)\n    LOG.info(f\"Exported: {SAVE_FILE} ({os.path.getsize(SAVE_FILE)/1024**2:.1f}MB)\")\nexcept Exception as e:\n    LOG.error(f\"Save Error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10) Visualization Helper Function","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_comparison(image, ground_truth, prediction, slice_idx=None):\n    if slice_idx is None:\n        slice_idx = image.shape[1] // 2 \n        \n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    \n    axes[0].imshow(image[0, slice_idx, :, :], cmap='gray')\n    axes[0].set_title(f'FLAIR MRI (Slice {slice_idx})')\n    axes[0].axis('off')\n    \n    axes[1].imshow(ground_truth[0, slice_idx, :, :], cmap='jet', alpha=0.8)\n    axes[1].set_title('Ground Truth (Whole Tumor)')\n    axes[1].axis('off')\n    \n    axes[2].imshow(prediction[0, slice_idx, :, :], cmap='jet', alpha=0.8)\n    axes[2].set_title('Model Prediction (Whole Tumor)')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11) Run Inference and Visualize","metadata":{}},{"cell_type":"code","source":"net.eval()\n\nwith torch.no_grad():\n    images, targets = next(iter(val_loader))\n    images = images.to(DEVICE)\n    \n    outputs, _, _ = net(images)\n    \n    preds = (torch.sigmoid(outputs) > 0.5).float().cpu()\n\nsample_img = images[0].cpu().numpy()\nsample_gt = targets[0].numpy()\nsample_pred = preds[0].numpy()\n\nvisualize_comparison(sample_img, sample_gt, sample_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12) Multimodal Visualizer","metadata":{}},{"cell_type":"code","source":"def plot_tumor_regions(gt, pred, slice_idx=64):\n    regions = ['Whole Tumor', 'Tumor Core', 'Enhancing Tumor']\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    for i in range(3):\n        axes[0, i].imshow(gt[i, slice_idx, :, :], cmap='gray')\n        axes[0, i].set_title(f'GT: {regions[i]}')\n        axes[0, i].axis('off')\n        \n        axes[1, i].imshow(pred[i, slice_idx, :, :], cmap='Reds')\n        axes[1, i].set_title(f'Pred: {regions[i]}')\n        axes[1, i].axis('off')\n    \n    plt.suptitle(f\"Segmentation Comparison - Slice {slice_idx}\")\n    plt.show()\n\nplot_tumor_regions(sample_gt, sample_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}