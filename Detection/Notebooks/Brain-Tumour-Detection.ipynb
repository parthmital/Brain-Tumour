{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":377107,"sourceType":"datasetVersion","datasetId":165566},{"sourceId":2809126,"sourceType":"datasetVersion","datasetId":740566},{"sourceId":9577119,"sourceType":"datasetVersion","datasetId":5838607}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Environment Setup","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -q \"protobuf<4.21.0\" tqdm lime shap scikit-image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:48:36.058437Z","iopub.execute_input":"2026-01-19T14:48:36.059232Z","iopub.status.idle":"2026-01-19T14:48:39.323626Z","shell.execute_reply.started":"2026-01-19T14:48:36.059202Z","shell.execute_reply":"2026-01-19T14:48:39.322735Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 2. Imports & Strategy","metadata":{}},{"cell_type":"code","source":"import os, cv2, warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n\nfrom sklearn.cluster import KMeans\nfrom skimage.feature import graycomatrix, graycoprops\nfrom skimage.segmentation import mark_boundaries\n\nfrom lime import lime_image\nimport shap\nfrom tqdm.keras import TqdmCallback\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nwarnings.filterwarnings(\"ignore\")\n\nstrategy = tf.distribute.MirroredStrategy()\nprint(\"GPUs:\", strategy.num_replicas_in_sync)\n\ntry:\n    from tensorflow.keras import mixed_precision\n    mixed_precision.set_global_policy(\"mixed_float16\")\n    print(\"Mixed precision enabled\")\nexcept Exception as e:\n    print(\"Mixed precision not enabled:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:48:39.326120Z","iopub.execute_input":"2026-01-19T14:48:39.326365Z","iopub.status.idle":"2026-01-19T14:48:39.339397Z","shell.execute_reply.started":"2026-01-19T14:48:39.326340Z","shell.execute_reply":"2026-01-19T14:48:39.338623Z"}},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\nGPUs: 2\nMixed precision enabled\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# 3. Dataset Paths","metadata":{}},{"cell_type":"code","source":"BR35H_ROOT = \"/kaggle/input/brain-tumor-detection\"\nBR35H_YES = os.path.join(BR35H_ROOT, \"yes\")\nBR35H_NO  = os.path.join(BR35H_ROOT, \"no\")\n\nNAV_ROOT = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection\"\nNAV_YES = os.path.join(NAV_ROOT, \"yes\")\nNAV_NO  = os.path.join(NAV_ROOT, \"no\")\n\nMOS_ROOT = \"/kaggle/input/brain-tumor-mri-yes-or-no/Brain (y-n)/Training\"\nMOS_YES = os.path.join(MOS_ROOT, \"yes\")\nMOS_NO  = os.path.join(MOS_ROOT, \"no\")\n\nprint(\"Train:\", BR35H_ROOT)\nprint(\"Val  :\", NAV_ROOT)\nprint(\"Test :\", \"/kaggle/input/brain-tumor-mri-yes-or-no\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:48:39.340409Z","iopub.execute_input":"2026-01-19T14:48:39.340667Z","iopub.status.idle":"2026-01-19T14:48:39.353580Z","shell.execute_reply.started":"2026-01-19T14:48:39.340640Z","shell.execute_reply":"2026-01-19T14:48:39.352947Z"}},"outputs":[{"name":"stdout","text":"Train: /kaggle/input/brain-tumor-detection\nVal  : /kaggle/input/brain-mri-images-for-brain-tumor-detection\nTest : /kaggle/input/brain-tumor-mri-yes-or-no\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# 4. Brain Cropping","metadata":{}},{"cell_type":"code","source":"def crop_brain(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)\n    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n    if len(cnts) == 0:\n        return cv2.resize(image, (224, 224))\n    c = max(cnts, key=cv2.contourArea)\n    x, y, w, h = cv2.boundingRect(c)\n    cropped = image[y:y+h, x:x+w]\n    return cv2.resize(cropped, (224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:48:39.355123Z","iopub.execute_input":"2026-01-19T14:48:39.355626Z","iopub.status.idle":"2026-01-19T14:48:39.365938Z","shell.execute_reply.started":"2026-01-19T14:48:39.355605Z","shell.execute_reply":"2026-01-19T14:48:39.365409Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# 5. Load Datasets","metadata":{}},{"cell_type":"code","source":"def load_images(folder, label):\n    imgs, lbls = [], []\n    for fname in os.listdir(folder):\n        path = os.path.join(folder, fname)\n        img = cv2.imread(path)\n        if img is None:\n            continue\n        img = crop_brain(img)\n        imgs.append(img)\n        lbls.append(label)\n    return imgs, lbls\n\ndef load_dataset(yes_dir, no_dir):\n    y_imgs, y_lbls = load_images(yes_dir, 1)\n    n_imgs, n_lbls = load_images(no_dir, 0)\n    X = np.array(y_imgs + n_imgs, dtype=np.uint8)\n    y = np.array(y_lbls + n_lbls, dtype=np.int32)\n    return X, y\n\nX_train_raw, y_train = load_dataset(BR35H_YES, BR35H_NO)\nX_val_raw, y_val = load_dataset(NAV_YES, NAV_NO)\nX_test_raw, y_test = load_dataset(MOS_YES, MOS_NO)\n\nprint(\"Train (Br35H):\", X_train_raw.shape, np.bincount(y_train))\nprint(\"Val (Navoneel):\", X_val_raw.shape, np.bincount(y_val))\nprint(\"Test (Mostafa):\", X_test_raw.shape, np.bincount(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:48:39.366824Z","iopub.execute_input":"2026-01-19T14:48:39.367107Z","iopub.status.idle":"2026-01-19T14:49:10.333726Z","shell.execute_reply.started":"2026-01-19T14:48:39.367088Z","shell.execute_reply":"2026-01-19T14:49:10.333019Z"}},"outputs":[{"name":"stdout","text":"Train (Br35H): (3000, 224, 224, 3) [1500 1500]\nVal (Navoneel): (253, 224, 224, 3) [ 98 155]\nTest (Mostafa): (5450, 224, 224, 3) [2725 2725]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# 6. Data Augmentation & Preprocessing","metadata":{}},{"cell_type":"code","source":"# 6. Data Augmentation & Preprocessing (UPDATED - NaN SAFE)\n\nBATCH = 32\nsteps_per_epoch = max(1, len(X_train_raw) // BATCH)\nprint(\"Batch:\", BATCH, \"Steps/epoch:\", steps_per_epoch)\n\n# Notes:\n# - cv2.imread loads BGR; ResNet preprocess expects RGB (it converts RGB->BGR internally).\n# - Random histogram matching is source-only (uses only training images) to randomize intensity styles.\n# - This version is NaN/Inf-safe and clips to [0, 255] before ResNet preprocess.\n\nfrom skimage.exposure import match_histograms\n\n_rng = np.random.default_rng(42)\n_ref_idx = _rng.choice(len(X_train_raw), size=min(256, len(X_train_raw)), replace=False)\n_ref_pool = X_train_raw[_ref_idx]\n\n\ndef _bgr_to_rgb_f32(x):\n    x = x.astype(np.float32)\n    return x[..., ::-1]\n\n\ndef _safe_clip_uint8_range(x):\n    # Remove NaN/Inf then clip to the intensity range expected by ImageNet preprocess.\n    x = np.nan_to_num(x, nan=0.0, posinf=255.0, neginf=0.0)\n    return np.clip(x, 0.0, 255.0).astype(np.float32)\n\n\ndef random_histogram_match_rgb(x_rgb: np.ndarray) -> np.ndarray:\n    ref = _ref_pool[_rng.integers(0, len(_ref_pool))]\n    ref_rgb = _bgr_to_rgb_f32(ref)\n\n    # Histogram match in float32, preserve_range, then sanitize.\n    y = match_histograms(x_rgb, ref_rgb, channel_axis=-1)\n    y = y.astype(np.float32)\n    y = _safe_clip_uint8_range(y)\n    return y\n\n\ndef preprocess_train(x):\n    # 1) Convert BGR->RGB\n    x = _bgr_to_rgb_f32(x)\n\n    # 2) Source-only style randomization via histogram matching\n    if _rng.random() < 0.25:\n        x = random_histogram_match_rgb(x)\n\n    # 3) Final safety (even if histogram match didn't run)\n    x = _safe_clip_uint8_range(x)\n\n    # 4) ResNet preprocessing (expects RGB input)\n    x = resnet_preprocess(x)\n    return x\n\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_train,\n    rotation_range=15,\n    width_shift_range=0.10,\n    height_shift_range=0.10,\n    zoom_range=0.20,\n    shear_range=0.10,\n    horizontal_flip=True,\n    brightness_range=(0.80, 1.20),\n    fill_mode=\"nearest\"\n)\n\ntrain_flow = datagen.flow(\n    X_train_raw, y_train,\n    batch_size=BATCH,\n    shuffle=True,\n    seed=42\n)\n\n# Validation/Test: do NOT histogram-match; just do correct color + ResNet preprocess.\nX_val_p  = resnet_preprocess(_safe_clip_uint8_range(X_val_raw[..., ::-1].astype(np.float32)))\nX_test_p = resnet_preprocess(_safe_clip_uint8_range(X_test_raw[..., ::-1].astype(np.float32)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:49:10.334653Z","iopub.execute_input":"2026-01-19T14:49:10.334877Z","iopub.status.idle":"2026-01-19T14:49:21.145927Z","shell.execute_reply.started":"2026-01-19T14:49:10.334859Z","shell.execute_reply":"2026-01-19T14:49:21.145328Z"}},"outputs":[{"name":"stdout","text":"Batch: 32 Steps/epoch: 93\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# 7. ResNet50 Binary Classifier","metadata":{}},{"cell_type":"code","source":"# 7. ResNet50 Binary Classifier (UPDATED)\n\nwith strategy.scope():\n    base = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n    base.trainable = False\n\n    x = layers.GlobalAveragePooling2D()(base.output)\n    x = layers.Dense(512, activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.6)(x)\n\n    out = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n\n    model = models.Model(inputs=base.input, outputs=out)\n\n    loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=[\n            \"accuracy\",\n            tf.keras.metrics.AUC(name=\"auc\")\n        ]\n    )\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Training","metadata":{}},{"cell_type":"code","source":"# 8. Training (UPDATED)\n\ncallbacks = [\n    EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True),\n    ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=3, factor=0.2, min_lr=1e-6),\n    ModelCheckpoint(\"best_resnet50_train_br35h.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True),\n    CSVLogger(\"training_train_br35h.csv\", append=False),\n    TqdmCallback(verbose=2)\n]\n\nprint(\"\\n=== Train on Br35H | Validate on Navoneel ===\")\nhistory = model.fit(\n    train_flow,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=(X_val_p, y_val),\n    epochs=30,\n    callbacks=callbacks,\n    verbose=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:49:22.966696Z","iopub.execute_input":"2026-01-19T14:49:22.966994Z","iopub.status.idle":"2026-01-19T14:57:47.185418Z","shell.execute_reply.started":"2026-01-19T14:49:22.966964Z","shell.execute_reply":"2026-01-19T14:57:47.184833Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ba43c991a84613aae662d9d85ddfd7"}},"metadata":{}},{"name":"stdout","text":"\n=== Train on Br35H | Validate on Navoneel ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd27b1d7132b4d63a2efbf38e9225caf"}},"metadata":{}},{"name":"stdout","text":"INFO:tensorflow:Collective all_reduce tensors: 6 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d9c8d7bf38c4765b4beceff40dc7c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7accdc029d1f4238b757280f6fbce9ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea59d12dfcfb491b8034df6bcaa9ff53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fda94120d8c440e182dc8be7545209d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3e2ee0873a4d68bfb1fc8644d8bb59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94425f0d451445fe8682acbb5e4b616e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8337799a7f8f494c96a63423576000df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94ff4d13bad4d409d868c4ce38e7913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"750bae179e12422398fcffe3055ba5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829bb87e380f4d308582748393a9249a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec9da9dfba0404e83247dbd889163bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f82234c707465b9c7604da00175f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389d270b880146d9ac28da758e82e345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e568b64fc54afbbb9e9bd9d5b7f192"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e47cdd7d6a4d0381e690c1d69fe7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca7ee6a97e34259a059c5261798adba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b200025818b94ea6b9fd1cb12a554190"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daddfd7b095543afb9247ff1f40c4342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2fc0c9a5d14eba80d33722c735c5c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1a28848c2f4321ba9b0576883480af"}},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# 9. Evaluate on Mostafa Test","metadata":{}},{"cell_type":"code","source":"# 9. Evaluate on Mostafa Test (UPDATED - NaN GUARD)\n\n# If something still goes wrong numerically, this will make it obvious.\nif not np.isfinite(X_test_p).all():\n    raise ValueError(\"X_test_p contains NaN/Inf\")\n\nloss, acc, auc = model.evaluate(X_test_p, y_test, verbose=1)\nprint(\"=\" * 50)\nprint(\"FINAL TEST (Mostafa)\")\nprint(\"=\" * 50)\nprint(f\"Test Accuracy : {acc:.4f}\")\nprint(f\"Test AUC      : {auc:.4f}\")\nprint(f\"Test Loss     : {loss:.4f}\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:57:47.186720Z","iopub.execute_input":"2026-01-19T14:57:47.186981Z","iopub.status.idle":"2026-01-19T14:58:10.985068Z","shell.execute_reply.started":"2026-01-19T14:57:47.186959Z","shell.execute_reply":"2026-01-19T14:58:10.984434Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.8268 - auc: 0.4095 - loss: 0.5316\n==================================================\nFINAL TEST (Mostafa)\n==================================================\nTest Accuracy : 0.7457\nTest AUC      : 0.8060\nTest Loss     : 0.8363\n==================================================\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"code","source":"# Fine Tuning (UPDATED)\n\nwith strategy.scope():\n    for layer in model.layers:\n        layer.trainable = False\n\n    for layer in model.layers:\n        if layer.name.startswith(\"conv5_\"):\n            layer.trainable = True\n\n    for layer in model.layers:\n        if isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = False\n\n    loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5),\n        loss=loss_fn,\n        metrics=[\n            \"accuracy\",\n            tf.keras.metrics.AUC(name=\"auc\")\n        ]\n    )\n\ncallbacks_ft = [\n    EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=4, restore_best_weights=True),\n    ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=2, factor=0.2, min_lr=1e-7),\n    ModelCheckpoint(\"best_resnet50_finetuned_conv5.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True),\n    CSVLogger(\"training_finetune_conv5.csv\", append=False),\n    TqdmCallback(verbose=2)\n]\n\nprint(\"\\n=== Fine-tune conv5_* | Train on Br35H | Validate on Navoneel ===\")\nhistory_ft = model.fit(\n    train_flow,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=(X_val_p, y_val),\n    epochs=15,\n    callbacks=callbacks_ft,\n    verbose=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:58:10.986856Z","iopub.execute_input":"2026-01-19T14:58:10.987175Z","iopub.status.idle":"2026-01-19T15:04:33.596622Z","shell.execute_reply.started":"2026-01-19T14:58:10.987153Z","shell.execute_reply":"2026-01-19T15:04:33.596098Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"0epoch [00:00, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e56b893f3e4e3b83fe01d5a4745876"}},"metadata":{}},{"name":"stdout","text":"\n=== Fine-tune conv5_* | Train on Br35H | Validate on Navoneel ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c30ff8d532843369d7bfe51c9df8d25"}},"metadata":{}},{"name":"stdout","text":"INFO:tensorflow:Collective all_reduce tensors: 20 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36d0419defc45f58b8e46a1833b8aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d1f3ec6a3a4bd7ae842e25f2ce154e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e321941ba4864d1281c9e50a82f734af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd067dab1cc40d487fa7f0bc1f104ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6edfb6ac83b4ff6821be0e7524e45d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b132f5e4ef4bf39caa828f535b1964"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317b8d4c2dec409a85d7eb7ed6547a56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0747c6584584416fa9db5290d646a5ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8c7ab39c3743adacf62d90643ec357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977b7942509844be9542cadbbcfaa9e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"210de836d6d349d195a6196ea31e67a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a8cc5d66d5843fbaf8c7918110dbb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824d9533d9d34a6f8bdf6bad55b8f800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/93.0 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d6ecfd9d994eff90ff9177781f64fd"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# Re-Test after Fine Tuning","metadata":{}},{"cell_type":"code","source":"# Re-Test after Fine Tuning (UPDATED)\n\nloss, acc, auc = model.evaluate(X_test_p, y_test, verbose=1)\nprint(\"=\" * 50)\nprint(\"FINAL TEST AFTER FINE-TUNING (Mostafa)\")\nprint(\"=\" * 50)\nprint(f\"Test Accuracy : {acc:.4f}\")\nprint(f\"Test AUC      : {auc:.4f}\")\nprint(f\"Test Loss     : {loss:.4f}\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T15:04:33.598274Z","iopub.execute_input":"2026-01-19T15:04:33.598550Z","iopub.status.idle":"2026-01-19T15:04:55.696397Z","shell.execute_reply.started":"2026-01-19T15:04:33.598531Z","shell.execute_reply":"2026-01-19T15:04:55.695656Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.8560 - auc: 0.4137 - loss: 0.5054\n==================================================\nFINAL TEST AFTER FINE-TUNING (Mostafa)\n==================================================\nTest Accuracy : 0.7628\nTest AUC      : 0.8144\nTest Loss     : 0.8889\n==================================================\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# 10. Grad-CAM","metadata":{}},{"cell_type":"code","source":"def make_gradcam_heatmap(img_batch_preprocessed, model, last_conv_layer_name=\"conv5_block3_out\"):\n    grad_model = tf.keras.models.Model(\n        inputs=model.inputs,\n        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, preds = grad_model(img_batch_preprocessed, training=False)\n        class_channel = preds[:, 0]\n\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)\n\n    heatmap = tf.maximum(heatmap, 0)\n    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-8)\n\n    return heatmap.numpy().astype(np.float32)\n\nidx = 0\nraw = X_test_raw[idx]\ninp = X_test_p[idx:idx+1]\n\nheatmap = make_gradcam_heatmap(inp, model, last_conv_layer_name=\"conv5_block3_out\")\nheatmap = cv2.resize(heatmap, (224, 224)).astype(np.float32)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(raw)\nplt.imshow(heatmap, alpha=0.4, cmap=\"jet\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. LIME","metadata":{}},{"cell_type":"code","source":"lime_explainer = lime_image.LimeImageExplainer()\n\ndef predict_fn_lime(images):\n    images = images.astype(np.float32)\n    images = resnet_preprocess(images)\n    p = model.predict(images, verbose=0).ravel()\n    return np.vstack([1 - p, p]).T\n\nidx = 0\nexplanation = lime_explainer.explain_instance(\n    X_test_raw[idx].astype(\"double\"),\n    predict_fn_lime,\n    top_labels=1,\n    num_samples=1000\n)\n\ntemp, mask = explanation.get_image_and_mask(\n    explanation.top_labels[0],\n    positive_only=True,\n    num_features=5\n)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(mark_boundaries(temp / 255.0, mask))\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. SHAP","metadata":{}},{"cell_type":"code","source":"idx = 0\n\nbg_idx = np.random.choice(X_train_raw.shape[0], 20, replace=False)\nbackground = resnet_preprocess(X_train_raw[bg_idx].astype(np.float32))\n\nx_uint8 = X_test_raw[idx:idx+1].astype(np.uint8)\nx_model = resnet_preprocess(x_uint8.astype(np.float32))\nx_plot = (x_uint8.astype(np.float32) / 255.0).clip(0, 1)\n\nexplainer = shap.GradientExplainer(model, background)\nshap_vals = explainer.shap_values(x_model)\n\nif isinstance(shap_vals, list):\n    shap_vals = shap_vals[0]\nelse:\n    shap_vals = shap_vals[..., 0] if shap_vals.ndim == 5 else shap_vals\n\nshap.image_plot([shap_vals], x_plot)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}